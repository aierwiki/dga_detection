{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_malicious():\n",
    "    import re\n",
    "    records = []\n",
    "    with open('./data/dga.txt') as f:\n",
    "        records = re.findall(r'(\\w+)\\t+([\\w.]+).*\\n', f.read())\n",
    "    df_malicious = pd.DataFrame({'Domain':[record[1] for record in records], 'Label':[record[0] for record in records]})\n",
    "    return df_malicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_benign():\n",
    "    df_benign = pd.read_csv('./data/top-1m.csv', index_col = 0, header = None)\n",
    "    df_benign.columns = ['Domain']\n",
    "    df_benign['Label'] = 'benign'\n",
    "    return df_benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    import tldextract\n",
    "    df_malicious = get_malicious()\n",
    "    df_benign = get_benign()\n",
    "    df_data = pd.concat([df_malicious, df_benign], axis = 0)\n",
    "    df_data['Target'] = df_data['Label'].map(lambda x : 0 if x == 'benign' else 1)\n",
    "    df_data['Domain'] = df_data['Domain'].map(lambda x : tldextract.extract(x).domain)\n",
    "    df_data = df_data.drop_duplicates(subset = ['Domain'])\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(df_data):\n",
    "    df_data_small = pd.concat([df_data[df_data['Target'] == 0].sample(500000), df_data[df_data['Target'] == 1].sample(300000)], axis = 0)\n",
    "    X = df_data_small['Domain'].values\n",
    "    y = df_data_small['Target'].values\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 2019, test_size = 0.2)\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    tokenizer = Tokenizer(char_level = True)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    X_train = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test = tokenizer.texts_to_sequences(X_test)\n",
    "    X_train = pad_sequences(X_train, padding = 'post')\n",
    "    X_test = pad_sequences(X_test, padding = 'post', maxlen = X_train.shape[1])\n",
    "    return X_train, X_test, y_train, y_test, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(words_num, max_length, feature_num):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Embedding, LSTM, Dense, Dropout, Activation\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = words_num, output_dim = feature_num, input_length = max_length))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    import keras.backend as K\n",
    "    def calc_recall_score(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "\n",
    "    def calc_precision_score(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "\n",
    "    def calc_f1_score(y_true, y_pred):\n",
    "        precision = calc_precision_score(y_true, y_pred)\n",
    "        recall = calc_recall_score(y_true, y_pred)\n",
    "        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', \n",
    "                  metrics = ['acc', calc_recall_score, calc_precision_score, calc_f1_score])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train, X_test, y_train, y_test, word_index):\n",
    "    model = build_model(len(word_index) + 1, X_train.shape[1], 128)\n",
    "    model.fit(X_train, y_train, batch_size = 128, epochs = 5, validation_split = 0.3)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, word_index = make_data(df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 448000 samples, validate on 192000 samples\n",
      "Epoch 1/5\n",
      "448000/448000 [==============================] - 473s 1ms/step - loss: 0.1648 - acc: 0.9372 - calc_recall_score: 0.8832 - calc_precision_score: 0.9243 - calc_f1_score: 0.9000 - val_loss: 0.0875 - val_acc: 0.9685 - val_calc_recall_score: 0.9209 - val_calc_precision_score: 0.9944 - val_calc_f1_score: 0.9557\n",
      "Epoch 2/5\n",
      "448000/448000 [==============================] - 473s 1ms/step - loss: 0.0581 - acc: 0.9817 - calc_recall_score: 0.9660 - calc_precision_score: 0.9850 - calc_f1_score: 0.9751 - val_loss: 0.0463 - val_acc: 0.9854 - val_calc_recall_score: 0.9774 - val_calc_precision_score: 0.9833 - val_calc_f1_score: 0.9802\n",
      "Epoch 3/5\n",
      "448000/448000 [==============================] - 474s 1ms/step - loss: 0.0438 - acc: 0.9863 - calc_recall_score: 0.9753 - calc_precision_score: 0.9882 - calc_f1_score: 0.9815 - val_loss: 0.0385 - val_acc: 0.9876 - val_calc_recall_score: 0.9734 - val_calc_precision_score: 0.9934 - val_calc_f1_score: 0.9831\n",
      "Epoch 4/5\n",
      "448000/448000 [==============================] - 475s 1ms/step - loss: 0.0368 - acc: 0.9884 - calc_recall_score: 0.9797 - calc_precision_score: 0.9894 - calc_f1_score: 0.9844 - val_loss: 0.0473 - val_acc: 0.9865 - val_calc_recall_score: 0.9730 - val_calc_precision_score: 0.9906 - val_calc_f1_score: 0.9815\n",
      "Epoch 5/5\n",
      "448000/448000 [==============================] - 474s 1ms/step - loss: 0.0333 - acc: 0.9898 - calc_recall_score: 0.9824 - calc_precision_score: 0.9904 - calc_f1_score: 0.9863 - val_loss: 0.0327 - val_acc: 0.9893 - val_calc_recall_score: 0.9788 - val_calc_precision_score: 0.9924 - val_calc_f1_score: 0.9854\n"
     ]
    }
   ],
   "source": [
    "model = train(X_train, X_test, y_train, y_test, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "class WeightedSelfAttention(keras.layers.Layer):\n",
    "    r\"\"\"Y = \\text{softmax}(XW + b) X\n",
    "    \n",
    "    See: https://arxiv.org/pdf/1708.00524.pdf\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, use_bias = True, return_attention = False, **kwargs):\n",
    "        super(WeightedSelfAttention, self).__init__(**kwargs)\n",
    "        self.supports_masking = True\n",
    "        self.use_bias = use_bias\n",
    "        self.return_attention = return_attention\n",
    "        self.W, self.b = None, None\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'use_bias' : self.use_bias,\n",
    "            'return_attention': self.return_attention\n",
    "        }\n",
    "        base_config = super(WeightedSelfAttention, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape = (int(input_shape[2]), 1),\n",
    "                                name = '{}_W'.format(self.name),\n",
    "                                initializer = keras.initializers.get('uniform'))\n",
    "        if self.use_bias:\n",
    "            self.b = self.add_weight(shape=(1,),\n",
    "                                    name = '{}_b'.format(self.name),\n",
    "                                    initializer = keras.initializers.get('zeros'))\n",
    "            super(WeightedSelfAttention, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x, mask = None):\n",
    "        logits = K.dot(x, self.W)\n",
    "        if self.use_bias:\n",
    "            logits += self.b\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis = -1, keepdims = True))\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        \n",
    "        att_weights = ai / (K.sum(ai, axis = 1, keepdims = True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis = 1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return input_shape[0], output_len\n",
    "    \n",
    "    def compute_mask(self, _, input_mask = None):\n",
    "        if self.return_attention:\n",
    "            return [None, None]\n",
    "        return None\n",
    "    \n",
    "    def get_custom_objects():\n",
    "        return {'WeightedSelfAttention':WeightedSelfAttention}\n",
    "    \n",
    "    \n",
    "def build_model_att(words_num, max_length, feature_num):\n",
    "    from keras.models import Sequential, Model\n",
    "    from keras.layers import Input, Embedding, LSTM, Dense, Dropout, Activation, Bidirectional\n",
    "    inputs = Input(shape = (max_length,), name = 'Input')\n",
    "    embd = Embedding(input_dim = words_num, output_dim = feature_num, input_length = max_length, name = 'Embedding')(inputs)\n",
    "    lstm = keras.layers.Bidirectional(LSTM(units = 128, return_sequences = True, name = 'Bi-LSTM'))(embd)\n",
    "    att = WeightedSelfAttention('Attention')(lstm)\n",
    "    outputs = Dense(1, activation = 'sigmoid', name = 'Output')(att)\n",
    "    model = Model(inputs = inputs, outputs = outputs)\n",
    "    \n",
    "    import keras.backend as K\n",
    "    def calc_recall_score(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "\n",
    "    def calc_precision_score(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "\n",
    "    def calc_f1_score(y_true, y_pred):\n",
    "        precision = calc_precision_score(y_true, y_pred)\n",
    "        recall = calc_recall_score(y_true, y_pred)\n",
    "        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
    "    \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', \n",
    "                  metrics = ['acc', calc_recall_score, calc_precision_score, calc_f1_score])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_att(X_train, X_test, y_train, y_test, word_index):\n",
    "    model = build_model_att(len(word_index) + 1, X_train.shape[1], 128)\n",
    "    hist = model.fit(X_train, y_train, batch_size = 128, epochs = 5, validation_split = 0.3)\n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 448000 samples, validate on 192000 samples\n",
      "Epoch 1/5\n",
      "448000/448000 [==============================] - 943s 2ms/step - loss: 0.1239 - acc: 0.9552 - calc_recall_score: 0.9215 - calc_precision_score: 0.9567 - calc_f1_score: 0.9367 - val_loss: 0.0615 - val_acc: 0.9796 - val_calc_recall_score: 0.9664 - val_calc_precision_score: 0.9788 - val_calc_f1_score: 0.9723\n",
      "Epoch 2/5\n",
      "448000/448000 [==============================] - 1017s 2ms/step - loss: 0.0511 - acc: 0.9835 - calc_recall_score: 0.9694 - calc_precision_score: 0.9865 - calc_f1_score: 0.9776 - val_loss: 0.0435 - val_acc: 0.9862 - val_calc_recall_score: 0.9777 - val_calc_precision_score: 0.9852 - val_calc_f1_score: 0.9812\n",
      "Epoch 3/5\n",
      "448000/448000 [==============================] - 1028s 2ms/step - loss: 0.0388 - acc: 0.9875 - calc_recall_score: 0.9775 - calc_precision_score: 0.9892 - calc_f1_score: 0.9831 - val_loss: 0.0389 - val_acc: 0.9881 - val_calc_recall_score: 0.9762 - val_calc_precision_score: 0.9919 - val_calc_f1_score: 0.9838\n",
      "Epoch 4/5\n",
      "448000/448000 [==============================] - 1006s 2ms/step - loss: 0.0327 - acc: 0.9896 - calc_recall_score: 0.9815 - calc_precision_score: 0.9907 - calc_f1_score: 0.9859 - val_loss: 0.0334 - val_acc: 0.9894 - val_calc_recall_score: 0.9817 - val_calc_precision_score: 0.9900 - val_calc_f1_score: 0.9857\n",
      "Epoch 5/5\n",
      "448000/448000 [==============================] - 1035s 2ms/step - loss: 0.0284 - acc: 0.9909 - calc_recall_score: 0.9841 - calc_precision_score: 0.9918 - calc_f1_score: 0.9878 - val_loss: 0.0318 - val_acc: 0.9897 - val_calc_recall_score: 0.9817 - val_calc_precision_score: 0.9907 - val_calc_f1_score: 0.9860\n"
     ]
    }
   ],
   "source": [
    "model, hist = train_with_att(X_train, X_test, y_train, y_test, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
